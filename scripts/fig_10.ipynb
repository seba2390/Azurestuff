{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-02T12:01:58.523847200Z",
     "start_time": "2024-02-02T12:01:55.672237700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "\n",
    "from src.CPQAOA import CP_QAOA\n",
    "from src.QAOA import QAOA\n",
    "from src.Chain import Chain\n",
    "from src.Tools import (portfolio_metrics, \n",
    "                       min_cost_partition, \n",
    "                       get_qubo, \n",
    "                       normalized_cost, \n",
    "                       qubo_limits, \n",
    "                       check_qubo)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def simulate(datapoint):\n",
    "    # Defining topology\n",
    "    my_chain = Chain(N_qubits=datapoint['N'])\n",
    "    my_chain.set_initialization_strategy(strategy=datapoint['init_strat'])\n",
    "    \n",
    "    # Deciding between grid and 1d chain topology\n",
    "    my_topology = my_chain\n",
    "    \n",
    "    # Generating random problem instance \n",
    "    expected_returns, covariances = portfolio_metrics(n=datapoint['N'], seed=datapoint['seed'])\n",
    "    \n",
    "    # Retrieving C_min, C_max and corresponding states for original portfolio problem\n",
    "    constrained_result, full_result, lmbda = min_cost_partition(nr_qubits=datapoint['N'],\n",
    "                                                                k=datapoint['k'],\n",
    "                                                                mu=expected_returns,\n",
    "                                                                sigma=covariances,\n",
    "                                                                alpha=datapoint['alpha'])\n",
    "    \n",
    "    portfolio_subspace_max_cost, portfolio_subspace_min_cost, portfolio_subspace_min_state = constrained_result['c_max'], constrained_result['c_min'], constrained_result['s']\n",
    "    #full_space_max_cost = full_result['c_max']\n",
    "    portfolio_subspace_min_state_str = ''.join([str(_) for _ in portfolio_subspace_min_state])\n",
    "\n",
    "    # Generating QUBO corresponding to current problem instance\n",
    "    Q, offset = get_qubo(mu=expected_returns,\n",
    "                         sigma=covariances, \n",
    "                         alpha=datapoint['alpha'],\n",
    "                         lmbda=lmbda+1e-8, # Adding small constant purposely\n",
    "                         k=datapoint['k'])\n",
    "    QUBO_limits = qubo_limits(Q=Q,offset=offset)\n",
    "    qubo_min_cost, qubo_max_cost = QUBO_limits['c_min'], QUBO_limits['c_max']\n",
    "    qubo_min_state, qubo_max_state = QUBO_limits['min_state'], QUBO_limits['max_state']\n",
    "    check_qubo(QUBO_matrix=Q, QUBO_offset=offset, expected_returns=expected_returns, covariances=covariances, alpha=datapoint['alpha'], k=datapoint['k'])\n",
    "    qubo_min_state_str = ''.join([str(_) for _ in qubo_min_state])\n",
    "\n",
    "    \n",
    "    if not portfolio_subspace_min_state_str == qubo_min_state_str:\n",
    "        raise RuntimeError(f'portfolio_subspace_min_state_str: {portfolio_subspace_min_state_str}, qubo_min_state_str={qubo_min_state_str}'+f'Min. cost of qubo is: {qubo_min_cost}, but min. cost of constrained portfolio is: {portfolio_subspace_min_cost}.')\n",
    "    \n",
    "    if not np.isclose(qubo_min_cost,portfolio_subspace_min_cost):\n",
    "        raise RuntimeError(f'Min. cost of qubo is: {qubo_min_cost}, but min. cost of constrained portfolio is: {portfolio_subspace_min_cost}.')\n",
    "    \n",
    "    if not qubo_max_cost >= portfolio_subspace_max_cost:\n",
    "        raise RuntimeError(f'Max. cost of qubo: {qubo_max_cost}, max. cost of portfolio subspace: {portfolio_subspace_max_cost} (should be qubo max. >= constrained portfolio max)')\n",
    "\n",
    "    \n",
    "    # Generating instances of ansatz'\n",
    "    CP_z_phase_ansatz = CP_QAOA(N_qubits=datapoint['N'],\n",
    "                     cardinality=datapoint['k'],\n",
    "                     layers=datapoint['layers'],\n",
    "                     topology=my_topology,\n",
    "                     QUBO_matrix=Q,\n",
    "                     with_next_nearest_neighbors=datapoint['w_nnn'],\n",
    "                     with_gradient=True,\n",
    "                     approximate_hamiltonian=True,\n",
    "                     with_z_phase=datapoint['w_z_phase'])\n",
    "    \n",
    "    NORMAL_ansatz = QAOA(N_qubits=datapoint['N'],\n",
    "                     layers=datapoint['layers'],\n",
    "                     QUBO_matrix=Q,\n",
    "                     QUBO_offset=offset,\n",
    "                     constraining_mixer=False,\n",
    "                     Topology=my_topology)\n",
    "    \n",
    "    # Choosing optimizer for scipy\n",
    "    available_methods = ['COBYLA', 'Nelder-Mead', 'BFGS']\n",
    "    optimizer_method = available_methods[0]\n",
    "    \n",
    "    # Generating callback function for plotting\n",
    "    CP_z_phase_costs = [] # Normalized costs\n",
    "    CP_z_phase_probs = [] # probability of optimal state\n",
    "    def CP_z_phase_callback_function(x):\n",
    "        _dict_ = CP_z_phase_ansatz.get_state_probabilities(flip_states=False)\n",
    "        # N.B. Normalizing w. respect to full space max cost\n",
    "        _cost_ = normalized_cost(result=_dict_,\n",
    "                                 QUBO_matrix=Q,\n",
    "                                 QUBO_offset=offset,\n",
    "                                 max_cost=portfolio_subspace_max_cost, \n",
    "                                 min_cost=qubo_min_cost)\n",
    "        if portfolio_subspace_min_state_str in list(_dict_.keys()):\n",
    "            CP_z_phase_probs.append(_dict_[portfolio_subspace_min_state_str])\n",
    "        else:\n",
    "            CP_z_phase_probs.append(0)\n",
    "        CP_z_phase_costs.append(_cost_)\n",
    "        \n",
    "    # Generating callback function for plotting\n",
    "    NORMAL_costs = [] # Normalized costs\n",
    "    NORMAL_probs = [] # probability of optimal state\n",
    "    def NORMAL_callback_function(x):\n",
    "        _dict_ = NORMAL_ansatz.get_state_probabilities(flip_states=False)\n",
    "        # N.B. Normalizing w. respect to full space max cost\n",
    "        _cost_ = normalized_cost(result=_dict_,\n",
    "                                 QUBO_matrix=Q,\n",
    "                                 QUBO_offset=offset,\n",
    "                                 max_cost=portfolio_subspace_max_cost, \n",
    "                                 min_cost=qubo_min_cost)\n",
    "        if portfolio_subspace_min_state_str in list(_dict_.keys()):\n",
    "            NORMAL_probs.append(_dict_[portfolio_subspace_min_state_str])\n",
    "        else:\n",
    "            NORMAL_probs.append(0)\n",
    "        NORMAL_costs.append(_cost_)\n",
    "    \n",
    "    # Generating initial guess for rotation angles\n",
    "    np.random.seed(datapoint['seed'])\n",
    "    theta_min, theta_max = -2*np.pi, 2*np.pi\n",
    "    N_angles = datapoint['layers'] * len(my_topology.get_NN_indices())\n",
    "    if datapoint['w_nnn']:\n",
    "        N_angles = datapoint['layers'] * len(my_topology.get_NNN_indices())\n",
    "    if datapoint['w_z_phase']:\n",
    "        N_angles += datapoint['N'] * datapoint['layers']\n",
    "    CP_z_phase_theta_i = np.random.uniform(low=theta_min, high=theta_max, size=N_angles)\n",
    "    np.random.seed(datapoint['seed'])\n",
    "    N_angles = 2 * datapoint['layers']\n",
    "    NORMAL_theta_i = np.random.uniform(low=theta_min, high=theta_max, size=N_angles)\n",
    "    \n",
    "    CP_z_phase_res = sc.optimize.minimize(fun=CP_z_phase_ansatz.get_cost, \n",
    "                                      x0=CP_z_phase_theta_i,\n",
    "                                      method=optimizer_method,\n",
    "                                      options={'disp': False, \n",
    "                                               'maxiter': datapoint['max_iter']},\n",
    "                                      callback=CP_z_phase_callback_function)\n",
    "    \n",
    "    NORMAL_res = sc.optimize.minimize(fun=NORMAL_ansatz.get_cost, \n",
    "                                      x0=NORMAL_theta_i,\n",
    "                                      method=optimizer_method,\n",
    "                                      options={'disp': False, \n",
    "                                               'maxiter': datapoint['max_iter']},\n",
    "                                      callback=NORMAL_callback_function)\n",
    "\n",
    "    return {'N': datapoint['layers'],\n",
    "            'k': datapoint['k'],\n",
    "            'alpha': datapoint['alpha'],\n",
    "            'max_iter': datapoint['max_iter'],\n",
    "            'topology': my_topology,\n",
    "            'seed': datapoint['seed'],\n",
    "            'next nearest neighbor': datapoint['w_nnn'],\n",
    "            'layers': datapoint['layers'],\n",
    "            'z-phase': datapoint['w_z_phase'],\n",
    "            'QAOA': (NORMAL_probs,NORMAL_costs,NORMAL_res),\n",
    "            'CP_VQA': (CP_z_phase_probs, CP_z_phase_costs, CP_z_phase_res)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T12:01:58.619321400Z",
     "start_time": "2024-02-02T12:01:58.604311500Z"
    }
   },
   "id": "1877d3e6c4a66e46",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=14)]: Done   1 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=14)]: Done   2 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=14)]: Done   3 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=14)]: Done   4 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=14)]: Done   5 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=14)]: Done   6 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=14)]: Done   7 tasks      | elapsed:   56.5s\n",
      "[Parallel(n_jobs=14)]: Done   8 tasks      | elapsed:   56.8s\n",
      "[Parallel(n_jobs=14)]: Done   9 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=14)]: Done  10 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=14)]: Done  11 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=14)]: Done  12 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=14)]: Done  13 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=14)]: Done  14 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=14)]: Done  15 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=14)]: Done  16 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=14)]: Done  17 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=14)]: Done  18 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=14)]: Done  19 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=14)]: Done  20 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=14)]: Done  21 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=14)]: Done  22 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=14)]: Done  23 tasks      | elapsed:  3.2min\n"
     ]
    }
   ],
   "source": [
    "# Number of Qubits \n",
    "N_vals = [3,4,5,6,7,8,9,10,11,12,13]\n",
    "# alpha in: s^T*mu + alpha*(s^T*Covar*s)\n",
    "alpha=0.5\n",
    "# Maximal number of iterations for classical solver\n",
    "max_iter=1500\n",
    "# Using next nearest neighbors (in 1D chain model)\n",
    "w_next_nearest_neighbors = True\n",
    "# Number of layer repetitions\n",
    "layer_dict = {3:1,\n",
    "              4:1,\n",
    "              5:2,\n",
    "              6:2,\n",
    "              7:2,\n",
    "              8:2,\n",
    "              9:2,\n",
    "              10:2,\n",
    "              11:3,\n",
    "              12:3,\n",
    "              13:3}\n",
    "\n",
    "\n",
    "initialization_strategies = [np.array([0,1,0]),\n",
    "                             np.array([0,1,0,1]),\n",
    "                             np.array([0,1,0,1,0]),\n",
    "                             np.array([0,1,0,1,0,1]),\n",
    "                             np.array([0,1,0,1,0,1,0]),\n",
    "                             np.array([0,1,0,1,0,1,0,1]),\n",
    "                             np.array([0,1,0,1,0,1,0,1,0]),\n",
    "                             np.array([0,1,0,1,0,1,0,1,0,1]),\n",
    "                             np.array([0,1,0,1,0,1,0,1,0,1,0]),\n",
    "                             np.array([0,1,0,1,0,1,0,1,0,1,0,1]),\n",
    "                             np.array([0,1,0,1,0,1,0,1,0,1,0,1,0])]\n",
    "\n",
    "\n",
    "N_SEEDS = 5\n",
    "N_jobs = os.cpu_count()-2\n",
    "\n",
    "datapoints = []\n",
    "for seed in range(N_SEEDS):\n",
    "    n_counter = 0\n",
    "    for N in N_vals:\n",
    "        k = N // 2\n",
    "        N_layers = layer_dict[N]\n",
    "        dp = {'N': N, 'k': k, 'alpha': alpha, 'max_iter': max_iter, 'seed': seed, 'w_nnn': w_next_nearest_neighbors, \n",
    "              'layers': N_layers, 'w_z_phase': False, 'init_strat': initialization_strategies[n_counter]}\n",
    "        datapoints.append(dp)\n",
    "        n_counter += 1\n",
    "        \n",
    "r = Parallel(n_jobs=N_jobs, verbose=51, backend='loky')(delayed(simulate)(datapoint) for datapoint in datapoints)\n",
    "\n",
    "for idx,result in enumerate(r):\n",
    "    filename = f'fig_10_results/res_{idx}.pkl'\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(result, file)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-02T13:24:10.067878Z"
    }
   },
   "id": "206a2a895a66dc64"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "results = []\n",
    "for filename in os.listdir('fig_10_results'):\n",
    "    if filename != '.DS_Store':\n",
    "        with open('fig_10_results/'+filename, 'rb') as file:\n",
    "            results.append(pickle.load(file))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T12:27:30.554485Z",
     "start_time": "2024-01-28T12:27:30.037925Z"
    }
   },
   "id": "7e371c67417606f3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heh = {'a':2}\n",
    "heh['a']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T11:06:49.619711600Z",
     "start_time": "2024-02-02T11:06:49.600187100Z"
    }
   },
   "id": "89fde2b834e0c03d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "66a03026340607ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
